import os
import json
import re
import numpy as np
import torch
import faiss
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from razdel import sentenize
import pymorphy3
from collections import Counter

# –ò–º–ø–æ—Ä—Ç –º–æ–¥—É–ª—è —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
from file_reader import read_file, get_supported_files, extract_metadata

# ==========================
# –ù–ê–°–¢–†–û–ô–ö–ò
# ==========================

DATA_DIR = "data"
MODEL_NAME = "intfloat/multilingual-e5-large"
BATCH_SIZE = 64
SIM_THRESHOLD = 0.80
MIN_SPECIES_OCC = 3

# OCR –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ (EasyOCR)
USE_OCR = True  # –í–∫–ª—é—á–∏—Ç—å OCR –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö PDF/DJVU
OCR_LANG = ["ru", "en"]  # –Ø–∑—ã–∫–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
USE_GPU_OCR = True  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU –¥–ª—è OCR (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
DPI_SCALE = 3.0  # –ú–∞—Å—à—Ç–∞–± —Ä–µ–Ω–¥–µ—Ä–∞ (3.0 = ~300 DPI)
MIN_TEXT_RATIO = 0.10  # ‚úÖ –ú–∏–Ω. –¥–æ–ª—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø—Ä–æ–ø—É—Å–∫–∞ OCR
OCR_PAGE_LIMIT = 500  # ‚úÖ –õ–∏–º–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è OCR –Ω–∞ —Ñ–∞–π–ª

device = "cuda" if torch.cuda.is_available() else "cpu"
print("CUDA –¥–æ—Å—Ç—É–ø–Ω–∞:", torch.cuda.is_available())
print("–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ:", device)

# ==========================
# –ú–û–î–ï–õ–ò
# ==========================

embedder = SentenceTransformer(MODEL_NAME, device=device)
if device == "cuda":
    embedder.half()

morph = pymorphy3.MorphAnalyzer()


# ==========================
# –§–£–ù–ö–¶–ò–ò –û–ë–†–ê–ë–û–¢–ö–ò –¢–ï–ö–°–¢–ê
# ==========================

def normalize_text(text):
    return re.sub(r'\s+', ' ', text).strip()



def lemmatize_phrase(text):
    words = re.findall(r"[–ê-–Ø–∞-—è–Å—ëA-Za-z\-]+", text)
    return " ".join(morph.parse(w)[0].normal_form for w in words)


def extract_candidate_species(text):
    pattern = r'\b[–ê-–Ø–Å][–∞-—è—ë]+(?:\s[–∞-—è—ë]+){0,3}'
    return re.findall(pattern, text)


def semantic_chunk(text):
    sentences = [s.text for s in sentenize(text)]
    if len(sentences) < 2:
        return sentences

    sent_emb = embedder.encode(
        ["passage: " + s for s in sentences],
        normalize_embeddings=True,
        batch_size=128
    )

    chunks = []
    current = [sentences[0]]

    for i in range(1, len(sentences)):
        sim = np.dot(sent_emb[i - 1], sent_emb[i])
        if sim < SIM_THRESHOLD:
            chunks.append(" ".join(current))
            current = [sentences[i]]
        else:
            current.append(sentences[i])

    if current:
        chunks.append(" ".join(current))

    return chunks


# ==========================
# –û–ë–†–ê–ë–û–¢–ö–ê –§–ê–ô–õ–û–í
# ==========================

all_chunks = []
metadata = []
species_counter = Counter()

files = get_supported_files(DATA_DIR)
print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(files)}")
print(f"   –§–æ—Ä–º–∞—Ç—ã: {set(os.path.splitext(f)[1] for f in files)}")

for file in tqdm(files, desc="–§–∞–π–ª—ã"):
    file_path = os.path.join(DATA_DIR, file)
    file_ext = os.path.splitext(file)[1].lower()

    # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å OCR –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
    raw_text = read_file(
        file_path,
        use_ocr=USE_OCR,
        ocr_lang=OCR_LANG,
        gpu=USE_GPU_OCR,
        dpi_scale=DPI_SCALE,
        min_text_ratio=MIN_TEXT_RATIO,  # ‚úÖ –î–æ–±–∞–≤—å—Ç–µ —ç—Ç–æ
        ocr_page_limit=OCR_PAGE_LIMIT   # ‚úÖ –∏ —ç—Ç–æ
    )

    if not raw_text or len(raw_text.strip()) < 50:
        print(f"   ‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω: –ø—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ—á–∏—Ç–∞–µ–º—ã–π")
        continue

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    file_meta = extract_metadata(file_path)

    # –û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    text = normalize_text(raw_text)

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–∏–¥–æ–≤ —Ä–∞—Å—Ç–µ–Ω–∏–π
    candidates = extract_candidate_species(text)
    for c in candidates:
        lemma = lemmatize_phrase(c)
        species_counter[lemma] += 1

    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ
    chunks = semantic_chunk(text)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    for chunk in chunks:
        all_chunks.append("passage: " + chunk)
        metadata.append({
            "source": file,
            "format": file_ext.lstrip("."),
            "title": file_meta.get("title"),
            "author": file_meta.get("author"),
            "length": len(chunk)
        })

    print(f"   ‚úÖ {file}: {len(chunks)} —á–∞–Ω–∫–æ–≤, {len(candidates)} –≤–∏–¥–æ–≤")

print(f"\nüìä –ò—Ç–æ–≥–æ —á–∞–Ω–∫–æ–≤: {len(all_chunks)}")
print(f"üìä –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–∏–¥–æ–≤: {len(species_counter)}")

# ==========================
# –í–ò–î–´
# ==========================

species_final = [
    sp for sp, count in species_counter.items()
    if count >= MIN_SPECIES_OCC
]

with open("species_list.json", "w", encoding="utf-8") as f:
    json.dump(species_final, f, ensure_ascii=False, indent=2)

print(f"üå± –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤–∏–¥–æ–≤ (‚â•{MIN_SPECIES_OCC} –≤—Ö–æ–∂–¥–µ–Ω–∏–π): {len(species_final)}")

# ==========================
# EMBEDDINGS
# ==========================

print("\nüî¢ –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
embeddings = embedder.encode(
    all_chunks,
    batch_size=BATCH_SIZE,
    normalize_embeddings=True,
    show_progress_bar=True
).astype("float32")

# ==========================
# FAISS INDEX (CPU)
# ==========================

dimension = embeddings.shape[1]
n_vectors = embeddings.shape[0]

print(f"\nüìê –í–µ–∫—Ç–æ—Ä–æ–≤: {n_vectors}, –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {dimension}")

if n_vectors < 5000:
    index = faiss.IndexFlatIP(dimension)
    print("‚úÖ –ò–Ω–¥–µ–∫—Å: IndexFlatIP (—Ç–æ—á–Ω—ã–π –ø–æ–∏—Å–∫)")
else:
    nlist = int(np.sqrt(n_vectors))
    nlist = max(32, min(nlist, 512))
    quantizer = faiss.IndexFlatIP(dimension)
    index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_INNER_PRODUCT)
    index.train(embeddings)
    print(f"‚úÖ –ò–Ω–¥–µ–∫—Å: IndexIVFFlat (nlist={nlist})")

index.add(embeddings)
faiss.write_index(index, "plants.index")
print("üíæ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: plants.index")

# ==========================
# –°–û–•–†–ê–ù–ï–ù–ò–ï –î–ê–ù–ù–´–•
# ==========================

with open("chunks.json", "w", encoding="utf-8") as f:
    json.dump(all_chunks, f, ensure_ascii=False, indent=2)

with open("metadata.json", "w", encoding="utf-8") as f:
    json.dump(metadata, f, ensure_ascii=False, indent=2)

print("üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: chunks.json, metadata.json")

# ==========================
# –¢–ï–°–¢ –ü–û–ò–°–ö–ê
# ==========================

print("\nüîç –¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞:")
test_queries = [
    "passage: —Ä–æ–∑–∞ —Å–∞–¥–æ–≤–∞—è",
    "passage: –ø–æ–º–∏–¥–æ—Ä—ã —É—Ö–æ–¥",
    "passage: –¥–µ—Ä–µ–≤—å—è –ø–ª–æ–¥–æ–≤—ã–µ"
]

for query in test_queries:
    q_emb = embedder.encode([query], normalize_embeddings=True)
    D, I = index.search(q_emb, 3)

    print(f"\n–ó–∞–ø—Ä–æ—Å: {query.replace('passage: ', '')}")
    for i, (dist, idx) in enumerate(zip(D[0], I[0])):
        if idx < len(all_chunks):
            text = all_chunks[idx].replace("passage: ", "")[:120]
            source = metadata[idx].get("source", "unknown")
            fmt = metadata[idx].get("format", "txt")
            print(f"  {i + 1}. {dist:.4f} | [{fmt}] {source}: {text}...")

print("\n‚úÖ INDEX READY üöÄ")
