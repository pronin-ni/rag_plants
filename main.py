import os
import json
import re
import numpy as np
import torch
import faiss
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from razdel import sentenize
import pymorphy3
from collections import Counter
import argparse

# –ò–º–ø–æ—Ä—Ç –º–æ–¥—É–ª—è —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
from file_reader import read_file, get_supported_files, extract_metadata

# ==========================
# –ù–ê–°–¢–†–û–ô–ö–ò
# ==========================
DATA_DIR = "data"
MODEL_NAME = "intfloat/multilingual-e5-large"
BATCH_SIZE = 64
SIM_THRESHOLD = 0.80
MIN_SPECIES_OCC = 3

# OCR –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
USE_OCR = True
OCR_LANG = ["ru", "en"]
USE_GPU_OCR = True
DPI_SCALE = 3.0
MIN_TEXT_RATIO = 0.10
OCR_PAGE_LIMIT = 1000

device = "cuda" if torch.cuda.is_available() else "cpu"
print("CUDA –¥–æ—Å—Ç—É–ø–Ω–∞:", torch.cuda.is_available())
print("–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ:", device)

# ==========================
# –ú–û–î–ï–õ–ò
# ==========================
embedder = SentenceTransformer(MODEL_NAME, device=device)
if device == "cuda":
    embedder.half()
morph = pymorphy3.MorphAnalyzer()

# ==========================
# –§–£–ù–ö–¶–ò–ò –û–ë–†–ê–ë–û–¢–ö–ò –¢–ï–ö–°–¢–ê
# ==========================
def normalize_text(text):
    return re.sub(r'\s+', ' ', text).strip()

def lemmatize_phrase(text):
    words = re.findall(r"[–ê-–Ø–∞-—è–Å—ëA-Za-z\-]+", text)
    return " ".join(morph.parse(w)[0].normal_form for w in words)

def extract_candidate_species(text):
    pattern = r'\b[–ê-–Ø–Å][–∞-—è—ë]+(?:\s[–∞-—è—ë]+){0,3}'
    return re.findall(pattern, text)

def semantic_chunk(text):
    sentences = [s.text for s in sentenize(text)]
    if len(sentences) < 2:
        return sentences
    sent_emb = embedder.encode(
        ["passage: " + s for s in sentences],
        normalize_embeddings=True,
        batch_size=128
    )
    chunks = []
    current = [sentences[0]]
    for i in range(1, len(sentences)):
        sim = np.dot(sent_emb[i - 1], sent_emb[i])
        if sim < SIM_THRESHOLD:
            chunks.append(" ".join(current))
            current = [sentences[i]]
        else:
            current.append(sentences[i])
    if current:
        chunks.append(" ".join(current))
    return chunks

# ==========================
# CLI + RESUME / CHECKPOINT
# ==========================
parser = argparse.ArgumentParser(description="–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è Plant RAG")
parser.add_argument("--force", action="store_true", help="–ü–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å –≤—Å—ë —Å –Ω—É–ª—è")
args = parser.parse_args()

CHUNKS_PATH = "chunks.json"
METADATA_PATH = "metadata.json"
SPECIES_PATH = "species_list.json"

resume = (os.path.exists(CHUNKS_PATH) and
          os.path.exists(METADATA_PATH) and
          os.path.exists(SPECIES_PATH) and not args.force)

if resume:
    print("‚úÖ –†–µ–∂–∏–º resume: –∑–∞–≥—Ä—É–∂–∞–µ–º –≥–æ—Ç–æ–≤—ã–µ —á–∞–Ω–∫–∏, –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ —Å–ø–∏—Å–æ–∫ –≤–∏–¥–æ–≤")
    with open(CHUNKS_PATH, encoding="utf-8") as f:
        all_chunks = json.load(f)
    with open(METADATA_PATH, encoding="utf-8") as f:
        metadata = json.load(f)
    with open(SPECIES_PATH, encoding="utf-8") as f:
        species_final = json.load(f)
    print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(all_chunks)} —á–∞–Ω–∫–æ–≤, {len(species_final)} –≤–∏–¥–æ–≤")
else:
    all_chunks = []
    metadata = []
    species_counter = Counter()
    files = get_supported_files(DATA_DIR)
    print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(files)}")
    print(f" –§–æ—Ä–º–∞—Ç—ã: {set(os.path.splitext(f)[1] for f in files)}")

    for file in tqdm(files, desc="–§–∞–π–ª—ã"):
        file_path = os.path.join(DATA_DIR, file)
        file_ext = os.path.splitext(file)[1].lower()
        raw_text = read_file(
            file_path,
            use_ocr=USE_OCR,
            ocr_lang=OCR_LANG,
            gpu=USE_GPU_OCR,
            dpi_scale=DPI_SCALE,
            min_text_ratio=MIN_TEXT_RATIO,
            ocr_page_limit=OCR_PAGE_LIMIT
        )
        if not raw_text or len(raw_text.strip()) < 50:
            print(f" ‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω: {file}")
            continue

        file_meta = extract_metadata(file_path)
        text = normalize_text(raw_text)

        candidates = extract_candidate_species(text)
        for c in candidates:
            lemma = lemmatize_phrase(c)
            species_counter[lemma] += 1

        chunks = semantic_chunk(text)
        for chunk in chunks:
            if len(chunk.strip()) < 50:
                continue
            all_chunks.append("passage: " + chunk)
            metadata.append({
                "source": file,
                "format": file_ext.lstrip("."),
                "title": file_meta.get("title"),
                "author": file_meta.get("author"),
                "length": len(chunk)
            })

        print(f" ‚úÖ {file}: {len(chunks)} —á–∞–Ω–∫–æ–≤, {len(candidates)} –≤–∏–¥–æ–≤")

    print(f"\nüìä –ò—Ç–æ–≥–æ —á–∞–Ω–∫–æ–≤: {len(all_chunks)}")
    print(f"üìä –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–∏–¥–æ–≤: {len(species_counter)}")

    # –í–ò–î–´
    species_final = [sp for sp, count in species_counter.items() if count >= MIN_SPECIES_OCC]
    species_final.sort()
    with open(SPECIES_PATH, "w", encoding="utf-8") as f:
        json.dump(species_final, f, ensure_ascii=False, indent=2)
    print(f"üå± –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤–∏–¥–æ–≤ (‚â•{MIN_SPECIES_OCC} –≤—Ö–æ–∂–¥–µ–Ω–∏–π): {len(species_final)}")

    # CHECKPOINT
    print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ...")
    with open(CHUNKS_PATH, "w", encoding="utf-8") as f:
        json.dump(all_chunks, f, ensure_ascii=False, indent=2)
    with open(METADATA_PATH, "w", encoding="utf-8") as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    print("‚úÖ Checkpoint —Å–æ—Ö—Ä–∞–Ω—ë–Ω!")

# ==========================
# EMBEDDINGS + NPY (–ü–†–ê–í–ò–õ–¨–ù–û–ï –ú–ï–°–¢–û!)
# ==========================
EMB_NPY_PATH = "all_embeddings.npy"
print("\nüî¢ –†–∞–±–æ—Ç–∞ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏...")

if os.path.exists(EMB_NPY_PATH):
    print(f"–ù–∞–π–¥–µ–Ω {EMB_NPY_PATH} ‚Üí –∑–∞–≥—Ä—É–∂–∞–µ–º")
    embeddings = np.load(EMB_NPY_PATH).astype("float32")
    if embeddings.shape[0] != len(all_chunks):
        print("!!! –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ: —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ ‚â† —á–∞–Ω–∫–æ–≤")
        print("–£–¥–∞–ª–∏—Ç–µ all_embeddings.npy –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –∑–∞–Ω–æ–≤–æ (—Å --force –µ—Å–ª–∏ –Ω—É–∂–Ω–æ)")
        raise ValueError("–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π")
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {embeddings.shape[0]} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ .npy")
else:
    print("–§–∞–π–ª .npy –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Üí –≤—ã—á–∏—Å–ª—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏")
    embeddings = embedder.encode(
        all_chunks,
        batch_size=BATCH_SIZE,
        normalize_embeddings=True,
        show_progress_bar=True
    ).astype("float32")
    print(f"–°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ ‚Üí {EMB_NPY_PATH}")
    np.save(EMB_NPY_PATH, embeddings)
    print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {embeddings.shape}")

# ==========================
# FAISS INDEX
# ==========================
dimension = embeddings.shape[1]
n_vectors = embeddings.shape[0]
print(f"\nüìê –í–µ–∫—Ç–æ—Ä–æ–≤: {n_vectors}, –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {dimension}")

if n_vectors < 5000:
    index = faiss.IndexFlatIP(dimension)
    print("‚úÖ –ò–Ω–¥–µ–∫—Å: IndexFlatIP (—Ç–æ—á–Ω—ã–π –ø–æ–∏—Å–∫)")
else:
    nlist = max(32, min(int(np.sqrt(n_vectors)), 512))
    quantizer = faiss.IndexFlatIP(dimension)
    index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_INNER_PRODUCT)
    index.train(embeddings)
    print(f"‚úÖ –ò–Ω–¥–µ–∫—Å: IndexIVFFlat (nlist={nlist})")

index.add(embeddings)

if isinstance(index, faiss.IndexIVFFlat):
    index.nprobe = 32
    print("‚úÖ nprobe = 32 (–ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞)")

faiss.write_index(index, "plants.index")
print("üíæ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: plants.index")

# ==========================
# –¢–ï–°–¢ –ü–û–ò–°–ö–ê
# ==========================
print("\nüîç –¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞:")
test_queries = ["—Ä–æ–∑–∞ —Å–∞–¥–æ–≤–∞—è", "–ø–æ–º–∏–¥–æ—Ä—ã —É—Ö–æ–¥", "–¥–µ—Ä–µ–≤—å—è –ø–ª–æ–¥–æ–≤—ã–µ"]
for query in test_queries:
    q_emb = embedder.encode(["query: " + query], normalize_embeddings=True).astype("float32")
    D, I = index.search(q_emb, 3)
    print(f"\n–ó–∞–ø—Ä–æ—Å: {query}")
    for i, (dist, idx) in enumerate(zip(D[0], I[0])):
        if idx < len(all_chunks):
            text = all_chunks[idx].replace("passage: ", "")[:120]
            source = metadata[idx].get("source", "unknown")
            fmt = metadata[idx].get("format", "txt")
            print(f" {i + 1}. {dist:.4f} | [{fmt}] {source}: {text}...")

print("\n‚úÖ INDEX READY üöÄ")